{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Convolutional Neural Networks with PyTorch\n",
    "## Applied Machine Learning\n",
    "### SmartGateML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le'ts see if we have any GPUs at our disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor(),\n",
    "                                          download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we've got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./data/\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ./data/\n",
      "    Transforms (if any): ToTensor()\n",
      "    Target Transforms (if any): None\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters. Please define them for me\n",
    "\n",
    "num_epochs = 5 \n",
    "batch_size = 50\n",
    "num_classes = 10\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader: Combines a dataset and a sampler, and provides a convenient iterators over the dataset.\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a Convolutional Neural Network with two convolutional layers, followed by two fully connected layers.\n",
    "\n",
    "nn.Module is the base class for all neural network modules in PyTorch. Our model should be a subclass of nn.Module this class.\n",
    "\n",
    "class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, ...)\n",
    "* in_channels (int): Number of channels in the input image\n",
    "* out_channels (int): Number of channels produced by the convolution\n",
    "* kernel_size (int or tuple): Size of the convolving kernel\n",
    "* stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
    "* padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\n",
    "\n",
    "This might help https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md to get intuition.\n",
    "\n",
    "**Please write a ConvNet module which contains**\n",
    "* 2 convolutioal layers with\n",
    "    * 16 filters/kernels, each having size 5 \n",
    "    * stride = 1\n",
    "    * padding = 2\n",
    "* each is followed bynormalise by \n",
    "    * batch normalisation\n",
    "    * ReLU non-linearity\n",
    "    * MaxPool layer with kernel_size=2, stride=2\n",
    "\n",
    "* 2 fully connected layers\n",
    "    1. x -> 64, followed by ReLU and Dropout (50% p)\n",
    "    2. 64 -> y\n",
    "    \n",
    "you may want to make use of nn.Sequential()    \n",
    "    \n",
    "Here is why one should use Batch-Norm:\n",
    "* Improves gradient flow, used on very deep models (Resnet need this)\n",
    "* Allow higher learning rates\n",
    "* Reduce dependency on initialization\n",
    "* Gives some kind of regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "          nn.Conv2d(1, 16, 3, 1, 1),\n",
    "          nn.BatchNorm2d(16),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2, 2))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "          nn.Conv2d(16, 32, 3, 1, 1),\n",
    "          nn.BatchNorm2d(32),\n",
    "          nn.ReLU(),\n",
    "          nn.MaxPool2d(2, 2))\n",
    "\n",
    "        self.fc1 = nn.Linear(7*7*32, 64)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        \n",
    "        # out.reshape(-1, 7*7*32)\n",
    "        out = out.view(-1, 7*7*32)\n",
    "        \n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.drop(out)\n",
    "        out = F.relu(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc1): Linear(in_features=1568, out_features=64, bias=True)\n",
      "  (drop): Dropout(p=0.5)\n",
      "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(num_classes).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "# Use CrossEntropyLoss and Adam\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer =  torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # [1/5], Step [50/1200], Loss: 1.3791\n",
      "Epoch # [1/5], Step [100/1200], Loss: 1.0802\n",
      "Epoch # [1/5], Step [150/1200], Loss: 0.7579\n",
      "Epoch # [1/5], Step [200/1200], Loss: 0.5601\n",
      "Epoch # [1/5], Step [250/1200], Loss: 0.3120\n",
      "Epoch # [1/5], Step [300/1200], Loss: 0.6409\n",
      "Epoch # [1/5], Step [350/1200], Loss: 0.1891\n",
      "Epoch # [1/5], Step [400/1200], Loss: 0.2286\n",
      "Epoch # [1/5], Step [450/1200], Loss: 0.2804\n",
      "Epoch # [1/5], Step [500/1200], Loss: 0.4529\n",
      "Epoch # [1/5], Step [550/1200], Loss: 0.2912\n",
      "Epoch # [1/5], Step [600/1200], Loss: 0.1741\n",
      "Epoch # [1/5], Step [650/1200], Loss: 0.1638\n",
      "Epoch # [1/5], Step [700/1200], Loss: 0.1219\n",
      "Epoch # [1/5], Step [750/1200], Loss: 0.2013\n",
      "Epoch # [1/5], Step [800/1200], Loss: 0.3407\n",
      "Epoch # [1/5], Step [850/1200], Loss: 0.1488\n",
      "Epoch # [1/5], Step [900/1200], Loss: 0.1614\n",
      "Epoch # [1/5], Step [950/1200], Loss: 0.1901\n",
      "Epoch # [1/5], Step [1000/1200], Loss: 0.1189\n",
      "Epoch # [1/5], Step [1050/1200], Loss: 0.1456\n",
      "Epoch # [1/5], Step [1100/1200], Loss: 0.1612\n",
      "Epoch # [1/5], Step [1150/1200], Loss: 0.2795\n",
      "Epoch # [1/5], Step [1200/1200], Loss: 0.1450\n",
      "Epoch # [2/5], Step [50/1200], Loss: 0.1210\n",
      "Epoch # [2/5], Step [100/1200], Loss: 0.2433\n",
      "Epoch # [2/5], Step [150/1200], Loss: 0.2247\n",
      "Epoch # [2/5], Step [200/1200], Loss: 0.1093\n",
      "Epoch # [2/5], Step [250/1200], Loss: 0.3444\n",
      "Epoch # [2/5], Step [300/1200], Loss: 0.2369\n",
      "Epoch # [2/5], Step [350/1200], Loss: 0.2463\n",
      "Epoch # [2/5], Step [400/1200], Loss: 0.0675\n",
      "Epoch # [2/5], Step [450/1200], Loss: 0.3002\n",
      "Epoch # [2/5], Step [500/1200], Loss: 0.2836\n",
      "Epoch # [2/5], Step [550/1200], Loss: 0.1972\n",
      "Epoch # [2/5], Step [600/1200], Loss: 0.1231\n",
      "Epoch # [2/5], Step [650/1200], Loss: 0.0438\n",
      "Epoch # [2/5], Step [700/1200], Loss: 0.1455\n",
      "Epoch # [2/5], Step [750/1200], Loss: 0.0902\n",
      "Epoch # [2/5], Step [800/1200], Loss: 0.2548\n",
      "Epoch # [2/5], Step [850/1200], Loss: 0.3220\n",
      "Epoch # [2/5], Step [900/1200], Loss: 0.1853\n",
      "Epoch # [2/5], Step [950/1200], Loss: 0.1153\n",
      "Epoch # [2/5], Step [1000/1200], Loss: 0.1936\n",
      "Epoch # [2/5], Step [1050/1200], Loss: 0.1856\n",
      "Epoch # [2/5], Step [1100/1200], Loss: 0.3105\n",
      "Epoch # [2/5], Step [1150/1200], Loss: 0.1388\n",
      "Epoch # [2/5], Step [1200/1200], Loss: 0.2184\n",
      "Epoch # [3/5], Step [50/1200], Loss: 0.2036\n",
      "Epoch # [3/5], Step [100/1200], Loss: 0.1180\n",
      "Epoch # [3/5], Step [150/1200], Loss: 0.0771\n",
      "Epoch # [3/5], Step [200/1200], Loss: 0.1154\n",
      "Epoch # [3/5], Step [250/1200], Loss: 0.1083\n",
      "Epoch # [3/5], Step [300/1200], Loss: 0.1054\n",
      "Epoch # [3/5], Step [350/1200], Loss: 0.2329\n",
      "Epoch # [3/5], Step [400/1200], Loss: 0.2158\n",
      "Epoch # [3/5], Step [450/1200], Loss: 0.0635\n",
      "Epoch # [3/5], Step [500/1200], Loss: 0.2340\n",
      "Epoch # [3/5], Step [550/1200], Loss: 0.0678\n",
      "Epoch # [3/5], Step [600/1200], Loss: 0.0446\n",
      "Epoch # [3/5], Step [650/1200], Loss: 0.1717\n",
      "Epoch # [3/5], Step [700/1200], Loss: 0.0940\n",
      "Epoch # [3/5], Step [750/1200], Loss: 0.1639\n",
      "Epoch # [3/5], Step [800/1200], Loss: 0.0150\n",
      "Epoch # [3/5], Step [850/1200], Loss: 0.0329\n",
      "Epoch # [3/5], Step [900/1200], Loss: 0.1729\n",
      "Epoch # [3/5], Step [950/1200], Loss: 0.0491\n",
      "Epoch # [3/5], Step [1000/1200], Loss: 0.2034\n",
      "Epoch # [3/5], Step [1050/1200], Loss: 0.2350\n",
      "Epoch # [3/5], Step [1100/1200], Loss: 0.2619\n",
      "Epoch # [3/5], Step [1150/1200], Loss: 0.0198\n",
      "Epoch # [3/5], Step [1200/1200], Loss: 0.1138\n",
      "Epoch # [4/5], Step [50/1200], Loss: 0.2165\n",
      "Epoch # [4/5], Step [100/1200], Loss: 0.0162\n",
      "Epoch # [4/5], Step [150/1200], Loss: 0.0878\n",
      "Epoch # [4/5], Step [200/1200], Loss: 0.0352\n",
      "Epoch # [4/5], Step [250/1200], Loss: 0.0925\n",
      "Epoch # [4/5], Step [300/1200], Loss: 0.0618\n",
      "Epoch # [4/5], Step [350/1200], Loss: 0.0859\n",
      "Epoch # [4/5], Step [400/1200], Loss: 0.1900\n",
      "Epoch # [4/5], Step [450/1200], Loss: 0.0308\n",
      "Epoch # [4/5], Step [500/1200], Loss: 0.1088\n",
      "Epoch # [4/5], Step [550/1200], Loss: 0.0185\n",
      "Epoch # [4/5], Step [600/1200], Loss: 0.0199\n",
      "Epoch # [4/5], Step [650/1200], Loss: 0.0787\n",
      "Epoch # [4/5], Step [700/1200], Loss: 0.1102\n",
      "Epoch # [4/5], Step [750/1200], Loss: 0.0781\n",
      "Epoch # [4/5], Step [800/1200], Loss: 0.0910\n",
      "Epoch # [4/5], Step [850/1200], Loss: 0.1076\n",
      "Epoch # [4/5], Step [900/1200], Loss: 0.0771\n",
      "Epoch # [4/5], Step [950/1200], Loss: 0.1378\n",
      "Epoch # [4/5], Step [1000/1200], Loss: 0.1451\n",
      "Epoch # [4/5], Step [1050/1200], Loss: 0.0152\n",
      "Epoch # [4/5], Step [1100/1200], Loss: 0.1874\n",
      "Epoch # [4/5], Step [1150/1200], Loss: 0.0217\n",
      "Epoch # [4/5], Step [1200/1200], Loss: 0.1727\n",
      "Epoch # [5/5], Step [50/1200], Loss: 0.0326\n",
      "Epoch # [5/5], Step [100/1200], Loss: 0.2769\n",
      "Epoch # [5/5], Step [150/1200], Loss: 0.1854\n",
      "Epoch # [5/5], Step [200/1200], Loss: 0.1506\n",
      "Epoch # [5/5], Step [250/1200], Loss: 0.0063\n",
      "Epoch # [5/5], Step [300/1200], Loss: 0.0375\n",
      "Epoch # [5/5], Step [350/1200], Loss: 0.2574\n",
      "Epoch # [5/5], Step [400/1200], Loss: 0.0509\n",
      "Epoch # [5/5], Step [450/1200], Loss: 0.1257\n",
      "Epoch # [5/5], Step [500/1200], Loss: 0.0593\n",
      "Epoch # [5/5], Step [550/1200], Loss: 0.1482\n",
      "Epoch # [5/5], Step [600/1200], Loss: 0.1399\n",
      "Epoch # [5/5], Step [650/1200], Loss: 0.0244\n",
      "Epoch # [5/5], Step [700/1200], Loss: 0.0145\n",
      "Epoch # [5/5], Step [750/1200], Loss: 0.0654\n",
      "Epoch # [5/5], Step [800/1200], Loss: 0.0425\n",
      "Epoch # [5/5], Step [850/1200], Loss: 0.1023\n",
      "Epoch # [5/5], Step [900/1200], Loss: 0.2634\n",
      "Epoch # [5/5], Step [950/1200], Loss: 0.1974\n",
      "Epoch # [5/5], Step [1000/1200], Loss: 0.2125\n",
      "Epoch # [5/5], Step [1050/1200], Loss: 0.0524\n",
      "Epoch # [5/5], Step [1100/1200], Loss: 0.1054\n",
      "Epoch # [5/5], Step [1150/1200], Loss: 0.0853\n",
      "Epoch # [5/5], Step [1200/1200], Loss: 0.0790\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Make consistent with the current device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # x, target = Variable(images), Variable(labels)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % batch_size == 0:\n",
    "            print ('Epoch # [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.83 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "\n",
    "model.eval()  \n",
    "\n",
    "# This sets the module in evaluation mode, which affects certain modules (e.g. BatchNorm, Dropout). \n",
    "\n",
    "with torch.no_grad(): # Why do we need this?\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        out = model(images)\n",
    "        _, p = torch.max(out.data, dim=1)\n",
    "        \n",
    "        total += labels.data.size()[0]\n",
    "        \n",
    "        correct += (p == labels.data).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_target:  tensor(7)\n",
      "test_image:  torch.Size([50, 1, 28, 28])\n",
      "test_prediction:  tensor(7)\n",
      "test_target:  tensor(7)\n",
      "test_image:  torch.Size([50, 1, 28, 28])\n",
      "test_prediction:  tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# Lets print a prediction\n",
    "pilTrans = torchvision.transforms.ToPILImage()\n",
    "\n",
    "# we're predicting on 50-sized batch, so our input to model should have 50x1x28x28 size\n",
    "def predict_single_image_label(test_image_idx: int) -> torch.FloatTensor:\n",
    "    test_image, test_target = test_dataset[test_image_idx]\n",
    "    \n",
    "    print(\"test_target: \", test_target_eighty)\n",
    "    print(\"test_image: \", test_image_eighty.size())\n",
    "    pilImg = pilTrans(test_image)\n",
    "    pilImg.show()\n",
    "    \n",
    "    test_image.unsqueeze_(-1)\n",
    "    test_image = test_image.expand(1,28,28,50)\n",
    "    test_image = test_image.transpose(3, 0)\n",
    "    test_image = test_image.transpose(3, 1)\n",
    "    test_image = test_image.transpose(3, 2)\n",
    "    # print(\"test_image as batch: \", test_image.size())\n",
    "\n",
    "    out = model(test_image)\n",
    "    _, predict = torch.max(out.data, dim=1)\n",
    "    return predict[0]\n",
    "\n",
    "print(\"test_prediction: \", predict_single_image_label(79))\n",
    "print(\"test_prediction: \", predict_single_image_label(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
